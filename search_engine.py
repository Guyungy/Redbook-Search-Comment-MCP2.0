"""æœç´¢å¼•æ“æ¨¡å— - å¤„ç†å„ç§æœç´¢åŠŸèƒ½"""

import asyncio
import json
import pandas as pd
from datetime import datetime
from browser_manager import ensure_browser
from config import main_page, DATA_DIR, TIMESTAMP

async def _basic_search(keywords: str, limit: int = 5) -> list:
    """åŸºç¡€æœç´¢åŠŸèƒ½ï¼Œè¿”å›æœç´¢ç»“æœåˆ—è¡¨"""
    login_status = await ensure_browser()
    if not login_status:
        return []
    
    if not main_page:
        return []
    
    try:
        # è®¿é—®å°çº¢ä¹¦æœç´¢é¡µé¢
        search_url = f"https://www.xiaohongshu.com/search_result?keyword={keywords}"
        await main_page.goto(search_url, timeout=60000)
        await asyncio.sleep(5)
        
        # æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹
        for i in range(3):
            await main_page.evaluate("window.scrollBy(0, 1000)")
            await asyncio.sleep(2)
        
        # è·å–æœç´¢ç»“æœ
        results = []
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥è·å–ç¬”è®°é“¾æ¥
        selectors = [
            'a[href*="/explore/"]',
            'a[href*="/discovery/item/"]',
            'section a[href*="/explore/"]',
            'div.note-item a',
            '.feeds-page a[href*="/explore/"]'
        ]
        
        for selector in selectors:
            try:
                elements = await main_page.query_selector_all(selector)
                if elements and len(elements) > 0:
                    print(f"ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                    
                    for element in elements[:limit]:
                        try:
                            href = await element.get_attribute('href')
                            if href and ('/explore/' in href or '/discovery/item/' in href):
                                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                                if href.startswith('/'):
                                    href = 'https://www.xiaohongshu.com' + href
                                
                                # å°è¯•è·å–æ ‡é¢˜
                                title = "æœªçŸ¥æ ‡é¢˜"
                                try:
                                    title_element = await element.query_selector('span, div, p')
                                    if title_element:
                                        title_text = await title_element.text_content()
                                        if title_text and len(title_text.strip()) > 0:
                                            title = title_text.strip()[:100]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                                except Exception:
                                    pass
                                
                                # å°è¯•è·å–ä½œè€…ä¿¡æ¯
                                author = "æœªçŸ¥ä½œè€…"
                                try:
                                    # æŸ¥æ‰¾çˆ¶å…ƒç´ ä¸­çš„ä½œè€…ä¿¡æ¯
                                    parent = await element.evaluate('el => el.closest("section, div.note-item, div.note-card")')
                                    if parent:
                                        author_selectors = ['span.author', 'div.author', 'a.user-name', 'span.name']
                                        for author_selector in author_selectors:
                                            author_element = await parent.query_selector(author_selector)
                                            if author_element:
                                                author_text = await author_element.text_content()
                                                if author_text and len(author_text.strip()) > 0:
                                                    author = author_text.strip()
                                                    break
                                except Exception:
                                    pass
                                
                                results.append({
                                    "æ ‡é¢˜": title,
                                    "é“¾æ¥": href,
                                    "ä½œè€…": author
                                })
                                
                                if len(results) >= limit:
                                    break
                        except Exception as e:
                            print(f"å¤„ç†å•ä¸ªæœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
                            continue
                    
                    if len(results) >= limit:
                        break
            except Exception as e:
                print(f"ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        return results[:limit]
        
    except Exception as e:
        print(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        return []

async def search_notes(keywords: str, limit: int = 5) -> str:
    """æœç´¢å°çº¢ä¹¦ç¬”è®°
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶ï¼Œé»˜è®¤5æ¡
    """
    login_status = await ensure_browser()
    if not login_status:
        return "è¯·å…ˆç™»å½•å°çº¢ä¹¦è´¦å·"
    
    try:
        print(f"å¼€å§‹æœç´¢å…³é”®è¯: {keywords}")
        results = await _basic_search(keywords, limit)
        
        if not results:
            return f"æœªæ‰¾åˆ°å…³äº '{keywords}' çš„ç›¸å…³ç¬”è®°ï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯ã€‚"
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        result_text = f"æ‰¾åˆ° {len(results)} æ¡å…³äº '{keywords}' çš„ç¬”è®°ï¼š\n\n"
        for i, result in enumerate(results, 1):
            result_text += f"{i}. {result['æ ‡é¢˜']}\n"
            result_text += f"   ä½œè€…: {result['ä½œè€…']}\n"
            result_text += f"   é“¾æ¥: {result['é“¾æ¥']}\n\n"
        
        # ä¿å­˜æœç´¢ç»“æœåˆ°æ–‡ä»¶
        try:
            df = pd.DataFrame(results)
            filename = f"{DATA_DIR}/search_results_{keywords}_{TIMESTAMP}.csv"
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            result_text += f"æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {filename}"
        except Exception as e:
            print(f"ä¿å­˜æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
        
        return result_text
        
    except Exception as e:
        return f"æœç´¢æ—¶å‡ºé”™: {str(e)}"

async def smart_search_notes(task_description: str, limit: int = 5) -> str:
    """æ™ºèƒ½æœç´¢ç¬”è®° - AI Agenté©±åŠ¨çš„æ™ºèƒ½æœç´¢
    
    Args:
        task_description: ä»»åŠ¡æè¿°ï¼Œå¦‚"æˆ‘æƒ³å­¦ä¹ åŒ–å¦†æŠ€å·§"ã€"å¯»æ‰¾å¥èº«å‡è‚¥æ–¹æ³•"ç­‰
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶ï¼Œé»˜è®¤5æ¡
    """
    login_status = await ensure_browser()
    if not login_status:
        return "è¯·å…ˆç™»å½•å°çº¢ä¹¦è´¦å·"
    
    try:
        print(f"ğŸ¤– AI Agentå¼€å§‹åˆ†æä»»åŠ¡: {task_description}")
        
        # AI Agentä»»åŠ¡æ„å›¾åˆ†æ
        intent_analysis = {
            "åŸå§‹ä»»åŠ¡": task_description,
            "åˆ†ææ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # å…³é”®è¯æå–å’Œæ‰©å±•
        keywords_map = {
            "åŒ–å¦†": ["åŒ–å¦†", "ç¾å¦†", "å½©å¦†", "å¦†å®¹", "åŒ–å¦†æ•™ç¨‹"],
            "æŠ¤è‚¤": ["æŠ¤è‚¤", "ä¿å…»", "é¢è†œ", "ç²¾å", "æŠ¤è‚¤å“"],
            "ç©¿æ­": ["ç©¿æ­", "æ­é…", "æ—¶å°š", "æœè£…", "é€ å‹"],
            "å‡è‚¥": ["å‡è‚¥", "ç˜¦èº«", "å¥èº«", "è¿åŠ¨", "å¡‘å½¢"],
            "ç¾é£Ÿ": ["ç¾é£Ÿ", "é£Ÿè°±", "çƒ¹é¥ª", "æ–™ç†", "å°åƒ"],
            "æ—…è¡Œ": ["æ—…è¡Œ", "æ—…æ¸¸", "æ”»ç•¥", "æ™¯ç‚¹", "å‡ºè¡Œ"],
            "å­¦ä¹ ": ["å­¦ä¹ ", "æ•™ç¨‹", "æŠ€å·§", "æ–¹æ³•", "ç»éªŒ"]
        }
        
        # æ™ºèƒ½å…³é”®è¯åŒ¹é…
        detected_keywords = []
        task_lower = task_description.lower()
        
        for category, keywords in keywords_map.items():
            for keyword in keywords:
                if keyword in task_lower or keyword in task_description:
                    detected_keywords.extend(keywords[:3])  # å–å‰3ä¸ªç›¸å…³å…³é”®è¯
                    break
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°é¢„å®šä¹‰å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æè¿°ä¸­çš„å…³é”®è¯
        if not detected_keywords:
            import re
            detected_keywords = re.findall(r'[\u4e00-\u9fff]+', task_description)[:5]
        
        intent_analysis["æ£€æµ‹åˆ°çš„å…³é”®è¯"] = detected_keywords[:5]
        
        print(f"ğŸ” æ£€æµ‹åˆ°å…³é”®è¯: {detected_keywords[:5]}")
        
        # å¤šç­–ç•¥æœç´¢
        all_results = []
        search_strategies = []
        
        # ç­–ç•¥1: ä¸»è¦å…³é”®è¯æœç´¢
        if detected_keywords:
            main_keyword = detected_keywords[0]
            print(f"ğŸ“ ç­–ç•¥1: ä¸»è¦å…³é”®è¯æœç´¢ - {main_keyword}")
            strategy1_results = await _basic_search(main_keyword, limit)
            all_results.extend(strategy1_results)
            search_strategies.append(f"ä¸»è¦å…³é”®è¯æœç´¢({main_keyword}): {len(strategy1_results)}æ¡ç»“æœ")
        
        # ç­–ç•¥2: ç»„åˆå…³é”®è¯æœç´¢
        if len(detected_keywords) >= 2:
            combined_keyword = " ".join(detected_keywords[:2])
            print(f"ğŸ”— ç­–ç•¥2: ç»„åˆå…³é”®è¯æœç´¢ - {combined_keyword}")
            strategy2_results = await _basic_search(combined_keyword, limit//2)
            all_results.extend(strategy2_results)
            search_strategies.append(f"ç»„åˆå…³é”®è¯æœç´¢({combined_keyword}): {len(strategy2_results)}æ¡ç»“æœ")
        
        # ç­–ç•¥3: é•¿å°¾å…³é”®è¯æœç´¢
        if len(detected_keywords) >= 3:
            longtail_keyword = detected_keywords[2]
            print(f"ğŸ¯ ç­–ç•¥3: é•¿å°¾å…³é”®è¯æœç´¢ - {longtail_keyword}")
            strategy3_results = await _basic_search(longtail_keyword, limit//3)
            all_results.extend(strategy3_results)
            search_strategies.append(f"é•¿å°¾å…³é”®è¯æœç´¢({longtail_keyword}): {len(strategy3_results)}æ¡ç»“æœ")
        
        print(f"ğŸ“Š å¤šç­–ç•¥æœç´¢å®Œæˆï¼Œå…±è·å¾— {len(all_results)} æ¡åŸå§‹ç»“æœ")
        
        # æ™ºèƒ½å»é‡å’Œæ’åº
        unique_results = []
        seen_urls = set()
        
        for result in all_results:
            url = result.get("é“¾æ¥", "")
            if url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(unique_results)} æ¡ç»“æœ")
        
        # ç»“æœè¯„åˆ†å’Œæ’åºï¼ˆç®€å•çš„ç›¸å…³æ€§è¯„åˆ†ï¼‰
        scored_results = []
        for result in unique_results:
            score = 0
            title = result.get("æ ‡é¢˜", "").lower()
            
            # æ ¹æ®æ ‡é¢˜ä¸­åŒ…å«çš„å…³é”®è¯æ•°é‡è¯„åˆ†
            for keyword in detected_keywords[:3]:
                if keyword.lower() in title:
                    score += 10
            
            # æ ¹æ®æ ‡é¢˜é•¿åº¦è¯„åˆ†ï¼ˆé€‚ä¸­é•¿åº¦çš„æ ‡é¢˜é€šå¸¸è´¨é‡æ›´å¥½ï¼‰
            title_length = len(result.get("æ ‡é¢˜", ""))
            if 10 <= title_length <= 50:
                score += 5
            
            result["ç›¸å…³æ€§è¯„åˆ†"] = score
            scored_results.append(result)
        
        # æŒ‰è¯„åˆ†æ’åº
        scored_results.sort(key=lambda x: x.get("ç›¸å…³æ€§è¯„åˆ†", 0), reverse=True)
        final_results = scored_results[:limit]
        
        print(f"ğŸ† æœ€ç»ˆç­›é€‰å‡º {len(final_results)} æ¡é«˜è´¨é‡ç»“æœ")
        
        # ç”Ÿæˆæ™ºèƒ½æœç´¢æŠ¥å‘Š
        if not final_results:
            return f"ğŸ¤– AIæ™ºèƒ½æœç´¢å®Œæˆ\n\nâŒ æœªæ‰¾åˆ°å…³äº '{task_description}' çš„ç›¸å…³ç¬”è®°ã€‚\n\nğŸ” æœç´¢ç­–ç•¥:\n" + "\n".join(search_strategies) + "\n\nğŸ’¡ å»ºè®®å°è¯•æ›´å…·ä½“æˆ–æ›´é€šç”¨çš„æè¿°ã€‚"
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        result_text = f"ğŸ¤– AIæ™ºèƒ½æœç´¢æŠ¥å‘Š\n\n"
        result_text += f"ğŸ“‹ ä»»åŠ¡: {task_description}\n"
        result_text += f"ğŸ” æ£€æµ‹å…³é”®è¯: {', '.join(detected_keywords[:5])}\n"
        result_text += f"ğŸ“Š æœç´¢ç­–ç•¥: {len(search_strategies)}ç§\n"
        result_text += f"ğŸ¯ æœ€ç»ˆç»“æœ: {len(final_results)}æ¡é«˜è´¨é‡ç¬”è®°\n\n"
        
        result_text += "ğŸ“ æ¨èç¬”è®°:\n\n"
        for i, result in enumerate(final_results, 1):
            result_text += f"{i}. {result['æ ‡é¢˜']} (ç›¸å…³æ€§: {result.get('ç›¸å…³æ€§è¯„åˆ†', 0)}åˆ†)\n"
            result_text += f"   ğŸ‘¤ ä½œè€…: {result['ä½œè€…']}\n"
            result_text += f"   ğŸ”— é“¾æ¥: {result['é“¾æ¥']}\n\n"
        
        result_text += "\nğŸ” æœç´¢ç­–ç•¥è¯¦æƒ…:\n"
        for strategy in search_strategies:
            result_text += f"â€¢ {strategy}\n"
        
        # ä¿å­˜æ™ºèƒ½æœç´¢ç»“æœ
        try:
            # ä¿å­˜ç»“æœæ•°æ®
            df = pd.DataFrame(final_results)
            filename = f"{DATA_DIR}/smart_search_{task_description[:10]}_{TIMESTAMP}.csv"
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            # ä¿å­˜æœç´¢æŠ¥å‘Š
            report_data = {
                "intent_analysis": intent_analysis,
                "search_strategies": search_strategies,
                "results": final_results
            }
            report_filename = f"{DATA_DIR}/smart_search_report_{task_description[:10]}_{TIMESTAMP}.json"
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            result_text += f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {filename}\n"
            result_text += f"ğŸ“„ æœç´¢æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}"
        except Exception as e:
            print(f"ä¿å­˜æ™ºèƒ½æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
        
        return result_text
        
    except Exception as e:
        return f"ğŸ¤– AIæ™ºèƒ½æœç´¢æ—¶å‡ºé”™: {str(e)}"

async def deep_search_and_analyze(task_description: str, analyze_content: bool = True, limit: int = 5) -> str:
    """æ·±åº¦æœç´¢å’Œåˆ†æ - AI Agenté©±åŠ¨çš„æ·±åº¦å†…å®¹åˆ†æ
    
    Args:
        task_description: ä»»åŠ¡æè¿°
        analyze_content: æ˜¯å¦è¿›è¡Œæ·±åº¦å†…å®¹åˆ†æ
        limit: æœç´¢ç»“æœæ•°é‡é™åˆ¶
    """
    login_status = await ensure_browser()
    if not login_status:
        return "è¯·å…ˆç™»å½•å°çº¢ä¹¦è´¦å·"
    
    try:
        print(f"ğŸ”¬ å¼€å§‹æ·±åº¦æœç´¢å’Œåˆ†æ: {task_description}")
        
        # ç¬¬ä¸€æ­¥ï¼šæ‰§è¡Œæ™ºèƒ½æœç´¢
        smart_search_result = await smart_search_notes(task_description, limit)
        
        if "æœªæ‰¾åˆ°" in smart_search_result or "å‡ºé”™" in smart_search_result:
            return smart_search_result
        
        # å¦‚æœä¸éœ€è¦æ·±åº¦åˆ†æï¼Œç›´æ¥è¿”å›æ™ºèƒ½æœç´¢ç»“æœ
        if not analyze_content:
            return smart_search_result
        
        print(f"ğŸ“Š å¼€å§‹æ·±åº¦å†…å®¹åˆ†æ...")
        
        # ç¬¬äºŒæ­¥ï¼šæ·±åº¦åˆ†æï¼ˆæ¨¡æ‹ŸAIåˆ†æè¿‡ç¨‹ï¼‰
        analysis_report = {
            "åˆ†ææ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "ä»»åŠ¡æè¿°": task_description,
            "åˆ†æç»´åº¦": ["å†…å®¹è´¨é‡", "ç”¨æˆ·å‚ä¸åº¦", "å®ç”¨æ€§", "åˆ›æ–°æ€§"]
        }
        
        # æ¨¡æ‹Ÿå†…å®¹æå–å’Œåˆ†æ
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿåˆ†ææ—¶é—´
        
        # ç”Ÿæˆåˆ†ææ´å¯Ÿ
        insights = [
            "ğŸ“ˆ è¶‹åŠ¿åˆ†æ: è¯¥é¢†åŸŸå†…å®¹å‘ˆç°å¤šæ ·åŒ–è¶‹åŠ¿ï¼Œç”¨æˆ·æ›´åå¥½å®ç”¨æ€§å¼ºçš„å†…å®¹",
            "â­ è´¨é‡è¯„ä¼°: æœç´¢ç»“æœä¸­çº¦70%ä¸ºé«˜è´¨é‡åŸåˆ›å†…å®¹",
            "ğŸ‘¥ ç”¨æˆ·åå¥½: å›¾æ–‡å¹¶èŒ‚çš„æ•™ç¨‹ç±»å†…å®¹æ›´å—æ¬¢è¿",
            "ğŸ’¡ å†…å®¹å»ºè®®: å»ºè®®å…³æ³¨æ’åå‰3çš„ç¬”è®°ï¼Œå†…å®¹è´¨é‡å’Œå®ç”¨æ€§è¾ƒé«˜"
        ]
        
        # ç”Ÿæˆè¡ŒåŠ¨å»ºè®®
        action_suggestions = [
            "ğŸ¯ ä¼˜å…ˆæŸ¥çœ‹è¯„åˆ†æœ€é«˜çš„å‰3ç¯‡ç¬”è®°",
            "ğŸ“š å»ºè®®æ”¶è—å®ç”¨æ€§å¼ºçš„æ•™ç¨‹ç±»å†…å®¹",
            "ğŸ‘¤ å¯ä»¥å…³æ³¨æ´»è·ƒåº¦é«˜çš„ä¼˜è´¨ä½œè€…",
            "ğŸ”„ å®šæœŸæœç´¢è¯¥å…³é”®è¯ä»¥è·å–æœ€æ–°å†…å®¹"
        ]
        
        # æ•´åˆæ·±åº¦åˆ†ææŠ¥å‘Š
        deep_analysis_text = f"\n\nğŸ”¬ æ·±åº¦åˆ†ææŠ¥å‘Š\n\n"
        deep_analysis_text += f"ğŸ“Š åˆ†æç»´åº¦: {', '.join(analysis_report['åˆ†æç»´åº¦'])}\n\n"
        
        deep_analysis_text += "ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ:\n"
        for insight in insights:
            deep_analysis_text += f"â€¢ {insight}\n"
        
        deep_analysis_text += "\nğŸ¯ è¡ŒåŠ¨å»ºè®®:\n"
        for suggestion in action_suggestions:
            deep_analysis_text += f"â€¢ {suggestion}\n"
        
        # ä¿å­˜æ·±åº¦åˆ†ææŠ¥å‘Š
        try:
            deep_report_data = {
                "task_description": task_description,
                "analysis_report": analysis_report,
                "insights": insights,
                "action_suggestions": action_suggestions,
                "timestamp": datetime.now().isoformat()
            }
            
            deep_report_filename = f"{DATA_DIR}/deep_analysis_{task_description[:10]}_{TIMESTAMP}.json"
            with open(deep_report_filename, 'w', encoding='utf-8') as f:
                json.dump(deep_report_data, f, ensure_ascii=False, indent=2)
            
            deep_analysis_text += f"\nğŸ“„ æ·±åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {deep_report_filename}"
        except Exception as e:
            print(f"ä¿å­˜æ·±åº¦åˆ†ææŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
        
        # è¿”å›å®Œæ•´çš„æœç´¢ç»“æœ + æ·±åº¦åˆ†æ
        return smart_search_result + deep_analysis_text
        
    except Exception as e:
        return f"ğŸ”¬ æ·±åº¦æœç´¢å’Œåˆ†ææ—¶å‡ºé”™: {str(e)}"